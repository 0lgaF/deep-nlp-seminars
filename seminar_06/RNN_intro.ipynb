{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the sake of reproducibility \n",
    "\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline\n",
    "\n",
    "* [RNN recap](#RNN-recap)\n",
    "    - [Task1](#Task1)\n",
    "    - [Dynamic RNN](#Dynamic-RNN)\n",
    "* [Generate names with RNN](#Name-generation)\n",
    "    - [Task2](#Task2)\n",
    "* [SRU implementation](#SRU-implementation)\n",
    "    - [Task3](#Task3)\n",
    "* [Bonus part](#Bonus-part)\n",
    "* [How to evaluate the work](#How-to-evaluate-the-work)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN recap\n",
    "\n",
    "<img src=\"./pics/rnn.png\" width=\"90%\">\n",
    "\n",
    "Simplest RNN consisting of 1 layer receives $x_{(t)}$ and could be written as:\n",
    "\n",
    "$$y_{(t)} = \\phi (x_{(t)}^T \\cdot w_x + y_{(t-1)}^T \\cdot w_y + b)$$\n",
    "\n",
    "where \n",
    "* $x(t)$ -- input vector at time step _t_ \n",
    "* $y(t)$ -- output vector at time step _t_\n",
    "* $w_x$ -- weights vector for input \n",
    "* $w_y$ -- weights vector for output\n",
    "* $y(t-1)$ -- output vector at previous time step; for 0th step it's zero vector\n",
    "* $b$ -- bias\n",
    "* $\\phi$ -- some activation function, i.e. ReLU\n",
    "\n",
    "\n",
    "Also we should mention **hidden_state** ( $h(t)$ ) -- it's a recurrent cell memory.\n",
    "\n",
    "In general case $h_{(t)} = f(h_{(t-1)}, x_{(t)})$, but also $y{(t)} = f(h{(t-1)}, x{(t)})$. So in this case $h(t) == y(t)$, but in practice more complex architectures are used, where **hidden_state** is not equal to the RNN output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "## Lets write simple RNN cell\n",
    "\n",
    "**[2 points max]**\n",
    "\n",
    "$$ h_{t} = \\tanh(x_{t} \\cdot W_x + h_{t-1} \\cdot W_h + b)$$\n",
    "where\n",
    "\n",
    "* $x_{t}$ -- [`batch_size` x `n_features`]\n",
    "* $h_{t}$ -- [`batch_size` x `hidden_dim`]\n",
    "* $x_{t}$ -- [`batch_size` x `n_features`]\n",
    "* $b$ -- vector of size `hidden_dim`\n",
    "* $W_x$ -- input weights of size [`n_features` x `hidden_dim`]\n",
    "* $W_h$ -- output weights of size [`hidden_dim` x `hidden_dim`]\n",
    "\n",
    "### Inline question 1\n",
    "\n",
    "What is RNN output size?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer here:** `<my_answer>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()  # just clear default graph and set seed for reproducibility\n",
    "\n",
    "weight_scale = 1e-3\n",
    "n_features = 3\n",
    "hidden_dim = 5\n",
    "\n",
    "# two time steps\n",
    "# the first dimension in shape parameter is None\n",
    "# for any batch size support\n",
    "\n",
    "X0 = tf.placeholder(tf.float32, shape=[None, n_features])\n",
    "X1 = tf.placeholder(tf.float32, shape=[None, n_features])\n",
    "\n",
    "############################################################################\n",
    "# TODO: Initialize the weights and biases of the RNN. Weights              #\n",
    "# should be initialized from a Gaussian centered at 0.0 with               #\n",
    "# standard deviation equal to weight_scale, and biases should be           #\n",
    "# initialized to zero. Weights and biases should be tf.Variables           #\n",
    "# Apply simple RNN cell twice: for X0 and X1. Store resulting tensor       #\n",
    "# operations as Y0 and Y1                                                  #\n",
    "############################################################################\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "Y0 = None\n",
    "Y1 = None\n",
    "\n",
    "############################################################################\n",
    "#                             END OF YOUR CODE                             #\n",
    "############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# mini-batches of size 4\n",
    "X0_batch = np.array([[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 0, 1]])  # time step 0 of mini-batch\n",
    "X1_batch = np.array([[9, 8, 7], [0, 0, 0], [6, 5, 4], [3, 2, 1]])  # time step 2 mini-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    Y0_val, Y1_val = sess.run([Y0, Y1], feed_dict={X0: X0_batch, X1: X1_batch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y0_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y1_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A few improvements\n",
    "\n",
    "**[1 point max]**\n",
    "\n",
    "Make the same computation using only one matrix multiplication per one step. Two big matricies sould be multiplied faster then four small matricies because of GPU parallelization magic.\n",
    "\n",
    "$$ h_{(t)} = \\tanh(x_{t} \\cdot W_x + h_{t-1} \\cdot W_h + b) = \\tanh([x_{t};  h_{t-1}] \\cdot W + b) $$\n",
    "where\n",
    "$$ W = [W_x^T W_h^T]^T $$\n",
    "\n",
    "*Square brackets mean concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()  # just clear default graph and set seed for reproducibility\n",
    "\n",
    "X0 = tf.placeholder(tf.float32, [None, n_features])\n",
    "X1 = tf.placeholder(tf.float32, [None, n_features])\n",
    "\n",
    "############################################################################\n",
    "# TODO: Initialize the weights and biases of the RNN. Weights              #\n",
    "# should be initialized from a Gaussian centered at 0.0 with               #\n",
    "# standard deviation equal to weight_scale, and biases should be           #\n",
    "# initialized to zero. Weights and biases should be tf.Variables           #\n",
    "# Apply simple RNN cell twice: for X0 and X1. Store resulting tensor       #\n",
    "# operations as Y0 and Y1                                                  #\n",
    "############################################################################\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "Y0 = None\n",
    "Y1 = None\n",
    "\n",
    "############################################################################\n",
    "#                             END OF YOUR CODE                             #\n",
    "############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    Y0_val_1, Y1_val_1 = sess.run([Y0, Y1], feed_dict={X0: X0_batch, X1: X1_batch})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic RNN\n",
    "\n",
    "In TensorFlow there is a function `tf.rnn.static_rnn` which create for each time step (unrolling) specific cell of desired type. Our implementation follows `tf.nn.rnn_cell.BasicRNNCell`. This implementation has such a drawback - we could need a lot of memory for long sequences. And because we want to work with such sequences we need to allocate a lot of memory at once. But in TF there is another option -- `dynamic_rnn`, where memory is allocated dynamically for each provided sequence, acording to its length.\n",
    "\n",
    "Lets rewrite the code with `dynamic_rnn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always in tensorflow the first step is writing a recipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 2\n",
    "n_features = 3\n",
    "hidden_dim = 5\n",
    "\n",
    "reset_graph() # just clear default graph and set seed for reproducibility\n",
    "\n",
    "# adding new parts to the default graph\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_features])\n",
    "\n",
    "# we have created the same cell in the Task1;\n",
    "basic_cell = tf.nn.rnn_cell.BasicRNNCell(num_units=hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = tf.placeholder(tf.int32, [None]) # create placeholder to feed in real values;\n",
    "\n",
    "# create dynamic_rnn and connect all existing graph components to it (i.e basic_cell, X, seq_length);\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32,\n",
    "                                    sequence_length=seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create matrices with real values in numpy.\n",
    "\n",
    "Notice that now `X_batch` have shape = `[None, n_steps, n_features]` that is **not the same** as in `Task1`.\n",
    "\n",
    "That's because of putting all time steps of batch instances in a single matrix `X_batch` (in the `Task1` we used two separated matrices `X0` and `X1` to feed values in each time stamp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_batch = np.array([\n",
    "        # step 0     step 1\n",
    "        [[0, 1, 2], [9, 8, 7]], # instance 1\n",
    "        [[3, 4, 5], [0, 0, 0]], # instance 2 (padded with zero vectors)\n",
    "        [[6, 7, 8], [6, 5, 4]], # instance 3\n",
    "        [[9, 0, 1], [3, 2, 1]], # instance 4\n",
    "    ])\n",
    "\n",
    "# sequence lengths\n",
    "seq_length_batch = np.array([2, 1, 2, 2]) # note the length of second instance is 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feed these real values into created network to get outputs and states values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new session context manager;\n",
    "# session will be closed as soon as this cell finish running\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run() # initialize all variables\n",
    "\n",
    "    # run session and feed input values into the network, get outputs and states values\n",
    "    outputs_val, states_val = sess.run(\n",
    "        [outputs, states], feed_dict={X: X_batch, seq_length: seq_length_batch})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of `outputs_val` is `[batch_size, time_steps, hidden_dim]` as it returns all outputs for each time step for each instance.\n",
    "\n",
    "The shape of `states_val` is `[batch_size, hidden_dim]` as it returns only last state for each instance of batch.\n",
    "\n",
    "__For the BasicRNNCell outputs and states are the same.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outputs_val.shape)\n",
    "print(states_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the second sample there are zeros in output \n",
    "print(outputs_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# but in state there are not\n",
    "print(states_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we feed `sequence_length` parameter into the `dynamic_rnn` we make `dynamic_rnn` to stop calculating states after actual sequence is ended. If we don't provide `sequence_length` parameter the calculating of states will continue and useful information about sequence could be lost if the padding is long enough. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name generation\n",
    "\n",
    "Lets try to do something fun with our RNNs.\n",
    "\n",
    "_Teaser:_\n",
    "\n",
    "* It is hard to choose a name for a variable. But its much harder to choose a name for a person.\n",
    "  So lets make neural net to do it instead!\n",
    "* Dataset consists of 8 thousand people names from different cultures all around the world.\n",
    "* Our toy task is training a model for name generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_token = \" \"\n",
    "\n",
    "with open(\"names\") as f:\n",
    "    names = f.readlines()\n",
    "    names = [start_token + name.lower() for name in names]\n",
    "    names = [name.strip() for name in names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('n samples = ', len(names))\n",
    "for x in names[::1000]:\n",
    "    print(x.capitalize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text processing\n",
    "\n",
    "Lets take all the latters disregarding a case + symbol ')' for the end of a name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_set = set()\n",
    "for name in names:\n",
    "    for letter in name:\n",
    "        token_set.add(letter)\n",
    "\n",
    "token_set.add(')')\n",
    "tokens = list(token_set)\n",
    "tokens.sort()\n",
    "\n",
    "print('n_tokens = ', len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_id = {t: i for i, t in enumerate(tokens)}\n",
    "\n",
    "id_to_token = {i: t for i, t in enumerate(tokens)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name length distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.hist(list(map(len, names)))\n",
    "\n",
    "# max length of a name in this dataset\n",
    "MAX_LEN = min([60, max(list(map(len, names)))])-1\n",
    "\n",
    "print(MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert symbols to their ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_ix = list(map(lambda name: list(map(token_to_id.get, name + ')')), names))\n",
    "\n",
    "\n",
    "for i in range(len(names_ix)):\n",
    "    names_ix[i] = names_ix[i][:MAX_LEN+1] #crop too long\n",
    "    \n",
    "    if len(names_ix[i]) < MAX_LEN+1:\n",
    "        names_ix[i] += [token_to_id[\" \"]]*(MAX_LEN+1 - len(names_ix[i])) #pad too short\n",
    "        \n",
    "assert len(set(map(len, names_ix))) == 1\n",
    "\n",
    "names_ix = np.array(names_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_ix[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_batch(data, batch_size):\n",
    "    \n",
    "    rows = data[np.random.randint(0, len(data), size=batch_size)]\n",
    "    x = rows[:, :-1]\n",
    "    y = rows[:, 1:]\n",
    "\n",
    "    count = lambda r: np.sum([id_to_token[t] != ' ' for t in r])\n",
    "    lengths = list(map(count, x))\n",
    "\n",
    "    return x, y, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, length = sample_batch(names_ix, 10)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network architecture and text generation process\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement the class `MyLittleNetwork` which will be used to generate sequences.\n",
    "\n",
    "<img src=\"https://vignette.wikia.nocookie.net/mlp/images/4/48/FANMADE_Rainbow_Dash_flying.png/revision/latest?cb=20121227194529\" width=\"100\" align=\"right\">\n",
    "\n",
    "Implemented class will have two useful properties:\n",
    "* Several instances of the class could live in one default graph thanks to using `tf.variable_scope()`\n",
    "* Each class instance could be created with its own recurrent cell type.\n",
    "\n",
    "These properties are useful for us as we want to compare several cell types by creating several class instances.\n",
    "\n",
    "**Outline of our work**\n",
    "\n",
    "1. **[[Build]](#Building-network-graph)** Creating network graph in `MyLittleNetwork.__init__` method\n",
    "2. **[[Train]](#Train-part)** Creating train procedure in `MyLittleNetwork.train` method\n",
    "3. **[[Infer]](#Sequence-generation)** Creating generation procedure in `MyLittleNetwork.generate_sample` method\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building network graph\n",
    "\n",
    "Consider method `__init__`. It takes several parameters that will be further discussed.\n",
    "\n",
    "```python\n",
    "...................................................\n",
    "def __init__(self, scope_name,\n",
    "             embedding_size = 8,\n",
    "             cell_class = tf.contrib.rnn.BasicRNNCell,\n",
    "             cell_params_dict = {'num_units': 60, 'activation':tf.tanh},\n",
    "             vocabulary_size = len(tokens)):\n",
    "...................................................         \n",
    "```\n",
    " \n",
    "Here `scope_name` just used to separate graph variables belonging to this particular instance of class `MyLittleNetwork`. `Tf.variable_scope` just add `scope_name` to the full name of all graph variables. In other words `tf.variable_scope` used for namespaces in Tf.\n",
    "We save `scope_name` parameter into `self.scope_name` to use this part of global default_graph along with a particular class instance.\n",
    "\n",
    "In the snippet below we just creating placeholders for inputs `_X`, targets `_y`, sequence length and learning_rate within `scope_name`. We do that using context manager `with tf.variable_scope`.\n",
    "```python\n",
    "...................................................\n",
    "self.scope_name = scope_name\n",
    "with tf.variable_scope(self.scope_name):\n",
    "    self._X = tf.placeholder(tf.int32, [None, None], name= 'X')\n",
    "    self._y = tf.placeholder(tf.int32, [None, None], name = 'y')\n",
    "    self._lengths = tf.placeholder(tf.int32, [None], name = 'lengths')\n",
    "    self._learning_rate_ph = tf.placeholder(dtype=tf.float32, shape=[], name = 'learning_rate_ph')\n",
    "...................................................\n",
    "```\n",
    "\n",
    "Look at the `_X` placeholder. It says you that it needs int32 input values. And there is no mistake.\n",
    "\n",
    "As input we will feed a sequence of numbers (it's our mapping numbers in dictionary). Actual shape of the `_X` is not defined yet and could be any. But actually it is `(batch_size, max_sequence_length)`.\n",
    "\n",
    "As you may remember from the part about [dynamic_rnn](#Dynamic-RNN) actually we feed into `dynamic_rnn` inputs with the shape `(batch_size, max_sequence_length, n_features)`. But how to get `n_features` dimension of inputs? Actually, we use `tf.embedding_lookup` function to map indices in `_X` to the vectors of embedding matrix. We put these obtained vectors to the `embed` variable.  And `_embedding_mtx` is just a usual `tf.Variable` with shape `(vocab_size, embedding_size)`. \n",
    "\n",
    "After obtaining embedding vectors for input `_X` we feed them into rnn cycle (i.e dynamic_rnn) which returns to us outputs and states (you may remember how it works from the [dynamic_rnn](#Dynamic-RNN) paragraph).   \n",
    "\n",
    "Actually we could use further either `rnn_outputs` or `states` or both to obtain logits. You can try different settings.\n",
    "But the most simple way is just using `rnn_outputs` as it contains information about each time step (and it is more then in `states`). So, use any option to obtain `_pred_logits` (i.e unnormalized scores for each token in the vocabulary).\n",
    "\n",
    "In the last line of the snippet below you have to translate input `_y` to one_hot representation using tf function.\n",
    "```python\n",
    "\n",
    "...................................................\n",
    "self._embedding_mtx = <create matrix of embeddings>\n",
    "embed = < embed the input sequence >\n",
    "\n",
    "self._cell = cell_class(**cell_params_dict)\n",
    "\n",
    "rnn_outputs, states = tf.nn.dynamic_rnn(< choose params >)\n",
    "self._pred_logits = < make logits >\n",
    "labels_one_hot = < create one_hot for targets self._y >\n",
    "...................................................\n",
    "```\n",
    "\n",
    "This is the last part of the architecture implementation.\n",
    "\n",
    "`tf.softmax_cross_entropy_with_logits` measures the probability error in discrete classification tasks in which the classes are mutually exclusive. The function calculate `softmax` under unnormilized logits entirely for efficiency. It returns 1-D Tensor of length `batch_size` of the same type as logits with the softmax cross entropy loss. That means it calculates loss for each instance in the batch separately for all vocabulary using formula: $$- \\Sigma y \\cdot log(\\hat{y})$$\n",
    "\n",
    "Since we have one-hot distribution for $y$ the resulted loss (for each instance in batch) takes into account only the logit value of the corresponding right token. Minimizing this loss leads us to maximizing the similarity between distributions of $y$ and it's estimate $\\hat{y}$.\n",
    "\n",
    "But in fact, loss must be scalar value, not tensor. That's why we apply `tf.reduce_mean` function next.\n",
    "\n",
    "Having loss function it is possible to take `AdamOptimizer` and minimize it (i.e calculate gradients giving a particular input and apply them to change network params). That's it and we will do that in the last line of this snippet. Besides we also define `_pred_probas` which is actually used only to generate sequence on the inference stage and doesn't need at train stage.\n",
    "\n",
    "```python\n",
    "....................................................\n",
    "self._stepwise_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "                                                        labels=labels_one_hot,\n",
    "                                                        logits=self._pred_logits)\n",
    "self._loss = tf.reduce_mean(self._stepwise_cross_entropy, name='loss')\n",
    "\n",
    "self._pred_probas = tf.nn.softmax(self._pred_logits, name='pred_probas')\n",
    "\n",
    "self._train_op = tf.train.AdamOptimizer(self._learning_rate_ph)\n",
    "                                        .minimize(self._loss, name='train_op')\n",
    "...................................................\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train part\n",
    "\n",
    "To make computations run we need to create new tf.Session or use existing one which hasn't been already closed.\n",
    "In this code snippet new session created but it used without context manager (i.e without `with tf.Session() ...`). It is useful to notice that fact as this session will be also used in the inference stage and we don't want to close it just after train finish.\n",
    "\n",
    "As always we initialize variables in this session in this variable scope. \n",
    "```python\n",
    "...................................................\n",
    "\n",
    "def train(self, n_epochs=10, batches_per_epoch = 500, batch_size = 10, lr = 1e-2):\n",
    "\n",
    "    losses = []\n",
    "    self.sess = tf.Session() \n",
    "    with tf.variable_scope(self.scope_name):\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "...................................................\n",
    "```\n",
    "\n",
    "In each epoch for each batch we run `_train_op` and get value of `_loss`.\n",
    "\n",
    "All the loss values collected into `losses` list which is returned at the end of training.\n",
    "\n",
    "See the next paragraph to understand `generate_sample` function.\n",
    "\n",
    "```python\n",
    "...................................................\n",
    "for epoch in range(n_epochs):\n",
    "    print(\">>Generated: \", self.generate_sample(n_snippets=6))\n",
    "    print(\"-------\\n\")\n",
    "    avg_cost = 0\n",
    "    for batch in range(batches_per_epoch):\n",
    "        x_, y_, len_ = sample_batch(names_ix, batch_size)\n",
    "\n",
    "        _, iloss = self.sess.run([self._train_op, self._loss],\n",
    "                                   {self._X: x_,\n",
    "                                    self._y: y_,\n",
    "                                    self._lengths: len_,\n",
    "                                    self._learning_rate_ph: lr})\n",
    "        avg_cost += iloss\n",
    "        losses.append(iloss)\n",
    "\n",
    "    print(\"EPOCH: \", epoch)\n",
    "    print(\"AVERAGE LOSS: \", avg_cost / batches_per_epoch)\n",
    "\n",
    "print(\">>Generated: \", self.generate_sample(n_snippets=6))\n",
    "...................................................\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence generation\n",
    "**Inference stage**\n",
    "\n",
    "<img src=\"http://tommymullaney.com/img/google-hangouts-feature.png\" width=\"400\">\n",
    "\n",
    "**How it works?**\n",
    "\n",
    "* Lets take seed phrase\n",
    "* Feeding it to the network\n",
    "* Predicting next token\n",
    "    * Next token is being sampled from model predicted distribution\n",
    "* Token is added to seed phrase\n",
    "* Repeat (from step 2)\n",
    "\n",
    "\n",
    "**`def generate_sample()`** in the *class `MyLittleNetwork`* actually do that. But it use `numpy` for sampling.\n",
    "So it actually run session to get probability distribution for the last token, then sample with `numpy` from that distribution to get next token. Token then added to the seed phrase and everything starts again from feeding phrase into the network. The picture greatly illustrate the process. Generation ends when the end token {here we use that token `)`} has been sampled or when the max length riched.\n",
    "\n",
    "\n",
    "It could be implemented more effectively using `tf.multinomial` and `tf.while_loop`.\n",
    "You could try to implement this function for generation sequences using tf only. This part of task is challenging and very optional.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name generation network architecture\n",
    "**[7 points max]** (4 points for adequate code and 3 points if generated names look like names)\n",
    "\n",
    "Add your code where necessary to create network architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "class MyLittleNetwork:\n",
    "    def __init__(self, scope_name,\n",
    "                 embedding_size = 8,\n",
    "                 cell_class = tf.contrib.rnn.BasicRNNCell,\n",
    "                 cell_params_dict = {'num_units': 60, 'activation':tf.tanh},\n",
    "                 vocabulary_size = len(tokens)):\n",
    "        \n",
    "        self.scope_name = scope_name\n",
    "        \n",
    "        with tf.variable_scope(self.scope_name):\n",
    "            \n",
    "            #################### PLACE FOR YOUR CODE  BELOW #########################\n",
    "            \n",
    "            self._X = tf.placeholder(tf.int32, [None, None], name= 'X')\n",
    "            self._y = tf.placeholder(tf.int32, [None, None], name = 'y')\n",
    "            self._lengths = tf.placeholder(tf.int32, [None], name = 'lengths')\n",
    "            self._learning_rate_ph = tf.placeholder(dtype=tf.float32, shape=[], name = 'learning_rate_ph')\n",
    "\n",
    "            self._embedding_mtx = <create matrix of embeddings>\n",
    "            embed = < embed the input sequence >\n",
    "            \n",
    "            self._cell = cell_class(**cell_params_dict)\n",
    "\n",
    "            rnn_outputs, states = tf.nn.dynamic_rnn(< choose params >)\n",
    "            self._pred_logits = < make logits >\n",
    "            labels_one_hot = < create one_hot for targets self._y >\n",
    "            \n",
    "            ##################### END OF YOUR TASK HERE ##############################\n",
    "\n",
    "            self._stepwise_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "                                                                    labels=labels_one_hot,\n",
    "                                                                    logits=self._pred_logits)\n",
    "\n",
    "            self._loss = tf.reduce_mean(self._stepwise_cross_entropy, name='loss')\n",
    "\n",
    "            self._pred_probas = tf.nn.softmax(self._pred_logits, name='pred_probas')\n",
    "\n",
    "            self._train_op = tf.train.AdamOptimizer(self._learning_rate_ph).\n",
    "                                                minimize(self._loss, name='train_op')\n",
    "\n",
    "    def generate_sample(self, seed_phrase=None, N=MAX_LEN, n_snippets=1):\n",
    "        \"\"\"\n",
    "        If you don't want to reimplement the function with tf\n",
    "                        don't touch it!\n",
    "        \"\"\"\n",
    "        if seed_phrase is None:\n",
    "            seed_phrase = ' '\n",
    "        elif seed_phrase[0].isalpha():\n",
    "            seed_phrase = ' ' + seed_phrase\n",
    "        seed_phrase = seed_phrase.lower()\n",
    "        seed_phrase = np.array([token_to_id[tok] for tok in seed_phrase])\n",
    "        L = len(seed_phrase)\n",
    "        snippets = []\n",
    "        \n",
    "        with tf.variable_scope(self.scope_name):\n",
    "            for _ in range(n_snippets):\n",
    "                x = np.zeros(N)\n",
    "                x[:len(seed_phrase)] = seed_phrase\n",
    "                for n in range(N - L):\n",
    "                    feed_dict = {self._X: x[:L + n].reshape([1, -1]), self._lengths: [len(x)]}\n",
    "                    p = self.sess.run(self._pred_probas[:, -1], feed_dict=feed_dict).reshape(-1)\n",
    "                    ix = np.random.choice(np.arange(len(tokens)), p=p)\n",
    "                    x[L + n] = ix\n",
    "                snippet = ''.join([id_to_token[idx] for idx in x])\n",
    "                if ')' in snippet:\n",
    "                    upto = snippet.index(')')\n",
    "                    snippet = snippet[:upto]\n",
    "                snippets.append(snippet.strip().capitalize())\n",
    "        return snippets\n",
    "\n",
    "    def train(self, n_epochs=10, batches_per_epoch = 500, batch_size = 10, lr = 1e-2):\n",
    "\n",
    "        losses = []\n",
    "        self.sess = tf.Session() \n",
    "        with tf.variable_scope(self.scope_name):\n",
    "            self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            for epoch in range(n_epochs):\n",
    "                print(\">>Generated: \", self.generate_sample(n_snippets=6))\n",
    "                print(\"-------\\n\")\n",
    "                avg_cost = 0\n",
    "                for batch in range(batches_per_epoch):\n",
    "                    x_, y_, len_ = sample_batch(names_ix, batch_size)\n",
    "\n",
    "                    _, iloss = self.sess.run([self._train_op, self._loss],\n",
    "                                               {self._X: x_,\n",
    "                                                self._y: y_,\n",
    "                                                self._lengths: len_,\n",
    "                                                self._learning_rate_ph: lr})\n",
    "                    avg_cost += iloss\n",
    "                    losses.append(iloss)\n",
    "\n",
    "                print(\"EPOCH: \", epoch)\n",
    "                print(\"AVERAGE LOSS: \", avg_cost / batches_per_epoch)\n",
    "\n",
    "            print(\">>Generated: \", self.generate_sample(n_snippets=6))\n",
    "        return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myBasicNN = MyLittleNetwork(scope_name=\"BasicRNNCell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which params in the network are trainable?\n",
    "\n",
    "Sometimes it is useful to look at trainable network parameters.\n",
    "\n",
    "* **for comparison**\n",
    " - A specially it is useful to compare one recurrent cell type to another.\n",
    "\n",
    "* **for sanity check**\n",
    " - Another reason is just to check is everything ok in your current default graph. Maybe there're redundant components which are unwanted. They may not be included into train procedure but may litter graph visualisation in tensorboard. Or maybe you forget to set `trainable=False` for your embedding matrix with pretrained embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.trainable_variables(scope=myBasicNN.scope_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets train it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**now train basic rnn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_hist_basic_rnn = myBasicNN.train(n_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time loss_hist_basic_rnn = myBasicNN.train(n_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time myBasicNN.generate_sample(seed_phrase='Puti', n_snippets=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myBasicNN.generate_sample(seed_phrase='Q', n_snippets=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myBasicNN.generate_sample(seed_phrase='Eug', n_snippets=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myBasicNN.generate_sample(seed_phrase='Lu', n_snippets=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**now lets train LSTM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myBasicLSTM = MyLittleNetwork(scope_name=\"BasicLSTMCell\", cell_class=tf.nn.rnn_cell.BasicLSTMCell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**check trainable params**\n",
    "\n",
    "If you look at shapes you will see that LSTM has more params then BasicRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.trainable_variables(scope=myBasicLSTM.scope_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -n 3 loss_hist_basic_lstm = myBasicLSTM.train(n_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time myBasicLSTM.generate_sample(seed_phrase='Puti', n_snippets=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_mean(x, N=1000):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
    "    return (cumsum[N:] - cumsum[:-N]) / float(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 5))\n",
    "plt.plot(running_mean(loss_hist_basic_rnn), label='BasicRNNCell', alpha=0.4)\n",
    "plt.plot(running_mean(loss_hist_basic_lstm), label='BasicLSTMCell', alpha=0.4)\n",
    "\n",
    "plt.title(\"Loss history\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus part\n",
    "### Do more interesting stuff\n",
    "\n",
    "**[extra points]**\n",
    "\n",
    "* Multi-layer (MultiRNNCell)\n",
    "* Generate tweets, using [this](http://study.mokoron.com) dataset\n",
    "* Gemerate texts using your favotive books / movie series / wikipedia articles as train set\n",
    "* Generate fibbonacci numbers\n",
    "* Measure maximum length of dependency (how much vanishing gradients affect RNN and LSTM), for example try to predict n-th previous symbol\n",
    "* Compare different initialization schemes and nonlinearities\n",
    "* Etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
